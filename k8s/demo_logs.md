```

k8s git:(main) âœ— make test-all
Running all tests... 

Test 1: Scale shards from 30 to 60
==========================================
Testing Shard Scaling Scenario
==========================================
Namespace: kds-test

This script will:
1. Update the Kinesis stream to 60 shards
2. Restart one worker pod to detect the change
3. Monitor the metadata updates

Step 1: Updating Kinesis stream to 60 shards...
Updating stream shard count to 60...
{
    "StreamName": "test-stream",
    "CurrentShardCount": 30,
    "TargetShardCount": 60
}


Step 2: Checking current metadata state BEFORE restart...
WORKER_ID                     MAX_LEASES  SHARD_COUNT  WORKER_COUNT  LAST_UPDATED
---------                     ----------  -----------  ------------  ------------
kds-consumer-2                10          30           3             2025-11-19T15:29:25Z
kds-consumer-app_coordinator  10          30           3             2025-11-19T15:29:04Z
kds-consumer-1                10          30           3             2025-11-19T15:29:13Z
kds-consumer-0                10          30           3             2025-11-19T15:29:04Z

Step 3: Performing rolling restart of all workers to detect the change...
statefulset.apps/kds-consumer restarted
Waiting for rolling restart to complete...
Waiting for partitioned roll out to finish: 0 out of 3 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 1 out of 3 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 2 out of 3 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
partitioned roll out complete: 3 new pods have been updated...

Step 4: Checking metadata state AFTER restart (may take a few seconds)...

Final Metadata State:
WORKER_ID                     MAX_LEASES  SHARD_COUNT  WORKER_COUNT  LAST_UPDATED
---------                     ----------  -----------  ------------  ------------
kds-consumer-2                20          60           3             2025-11-19T15:30:02Z
kds-consumer-app_coordinator  20          60           3             2025-11-19T15:30:02Z
kds-consumer-1                20          60           3             2025-11-19T15:30:17Z
kds-consumer-0                20          60           3             2025-11-19T15:30:32Z

==========================================
Test Complete!
==========================================

Summary:
  New shard count: 60
  Check max_leases_per_worker in the table above
  Expected: min(80, ceil(60 / worker_count))


Test 2: Scale workers from 3 to 5
==========================================
Testing Worker Scaling Scenario
==========================================
Namespace: kds-test

This script will:
1. Scale the StatefulSet to 5 replicas
2. Wait for new pods to be ready
3. Monitor the metadata updates

Step 1: Current metadata state BEFORE scaling...
WORKER_ID                     MAX_LEASES  SHARD_COUNT  WORKER_COUNT  LAST_UPDATED
---------                     ----------  -----------  ------------  ------------
kds-consumer-2                20          60           3             2025-11-19T15:30:02Z
kds-consumer-app_coordinator  20          60           3             2025-11-19T15:30:02Z
kds-consumer-1                20          60           3             2025-11-19T15:30:17Z
kds-consumer-0                20          60           3             2025-11-19T15:30:32Z

Step 2: Scaling StatefulSet to 5 replicas...
statefulset.apps/kds-consumer scaled

Step 3: Waiting for all pods to be ready...
pod/kds-consumer-0 condition met
pod/kds-consumer-1 condition met
pod/kds-consumer-2 condition met
pod/kds-consumer-3 condition met

Step 4: Performing rolling restart of all workers to detect the change...
statefulset.apps/kds-consumer restarted
Waiting for rolling restart to complete...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 0 out of 5 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 1 out of 5 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 2 out of 5 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 3 out of 5 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 4 out of 5 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
partitioned roll out complete: 5 new pods have been updated...

Giving workers time to initialize and detect changes...

Step 5: Checking metadata state AFTER scaling...

Final Metadata State:
WORKER_ID                     MAX_LEASES  SHARD_COUNT  WORKER_COUNT  LAST_UPDATED
---------                     ----------  -----------  ------------  ------------
kds-consumer-2                12          60           5             2025-11-19T15:31:58Z
kds-consumer-app_coordinator  12          60           5             2025-11-19T15:31:00Z
kds-consumer-4                12          60           5             2025-11-19T15:31:27Z
kds-consumer-1                12          60           5             2025-11-19T15:32:13Z
kds-consumer-3                12          60           5             2025-11-19T15:31:42Z
kds-consumer-0                12          60           5             2025-11-19T15:32:28Z

==========================================
Test Complete!
==========================================

Summary:
  New worker count: 5
  Check max_leases_per_worker in the table above
  Expected: min(80, ceil(shard_count / 5))
```